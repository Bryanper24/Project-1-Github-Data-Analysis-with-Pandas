{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMi4OTk8AsTfdxIucsu0iPf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bryanper24/Project-1-Github-Data-Analysis-with-Pandas/blob/main/Copy_of_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdQENbW6lHDF",
        "outputId": "57120af3-b8ea-4d04-e9cb-a4466013188d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running built-in test cases...\n",
            "============================================================\n",
            "TEST CASE: test1.rat\n",
            "Source:\n",
            "/* Compute the average of two real numbers */\n",
            "function average(a, b) {\n",
            "    real result;\n",
            "    result = (a + b) / 2.00;\n",
            "    return result;\n",
            "}\n",
            "\n",
            "token           lexeme\n",
            "-----------------------------------\n",
            "keyword         function\n",
            "identifier      average\n",
            "separator       (\n",
            "identifier      a\n",
            "separator       ,\n",
            "identifier      b\n",
            "separator       )\n",
            "separator       {\n",
            "keyword         real\n",
            "identifier      result\n",
            "separator       ;\n",
            "identifier      result\n",
            "operator        =\n",
            "separator       (\n",
            "identifier      a\n",
            "operator        +\n",
            "identifier      b\n",
            "separator       )\n",
            "operator        /\n",
            "real            2.00\n",
            "separator       ;\n",
            "keyword         return\n",
            "identifier      result\n",
            "separator       ;\n",
            "separator       }\n",
            "\n",
            "[Written to test_outputs/test1_output.txt]\n",
            "\n",
            "============================================================\n",
            "TEST CASE: test2.rat\n",
            "Source:\n",
            "/* Count down from n to zero */\n",
            "integer n;\n",
            "n = 10;\n",
            "while (n > 0) {\n",
            "    put(n);\n",
            "    n = n - 1;\n",
            "}\n",
            "endwhile\n",
            "\n",
            "token           lexeme\n",
            "-----------------------------------\n",
            "keyword         integer\n",
            "identifier      n\n",
            "separator       ;\n",
            "identifier      n\n",
            "operator        =\n",
            "integer         10\n",
            "separator       ;\n",
            "keyword         while\n",
            "separator       (\n",
            "identifier      n\n",
            "operator        >\n",
            "integer         0\n",
            "separator       )\n",
            "separator       {\n",
            "keyword         put\n",
            "separator       (\n",
            "identifier      n\n",
            "separator       )\n",
            "separator       ;\n",
            "identifier      n\n",
            "operator        =\n",
            "identifier      n\n",
            "operator        -\n",
            "integer         1\n",
            "separator       ;\n",
            "separator       }\n",
            "keyword         endwhile\n",
            "\n",
            "[Written to test_outputs/test2_output.txt]\n",
            "\n",
            "============================================================\n",
            "TEST CASE: test3.rat\n",
            "Source:\n",
            "/* Grade checker: checks score and prints pass or fail */\n",
            "integer score;\n",
            "boolean passed;\n",
            "get(score);\n",
            "if (score >= 60) {\n",
            "    passed = true;\n",
            "} else {\n",
            "    passed = false;\n",
            "}\n",
            "endif\n",
            "put(passed);\n",
            "\n",
            "token           lexeme\n",
            "-----------------------------------\n",
            "keyword         integer\n",
            "identifier      score\n",
            "separator       ;\n",
            "keyword         boolean\n",
            "identifier      passed\n",
            "separator       ;\n",
            "keyword         get\n",
            "separator       (\n",
            "identifier      score\n",
            "separator       )\n",
            "separator       ;\n",
            "keyword         if\n",
            "separator       (\n",
            "identifier      score\n",
            "operator        >=\n",
            "integer         60\n",
            "separator       )\n",
            "separator       {\n",
            "identifier      passed\n",
            "operator        =\n",
            "keyword         true\n",
            "separator       ;\n",
            "separator       }\n",
            "keyword         else\n",
            "separator       {\n",
            "identifier      passed\n",
            "operator        =\n",
            "keyword         false\n",
            "separator       ;\n",
            "separator       }\n",
            "keyword         endif\n",
            "keyword         put\n",
            "separator       (\n",
            "identifier      passed\n",
            "separator       )\n",
            "separator       ;\n",
            "\n",
            "[Written to test_outputs/test3_output.txt]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\"\"\"\n",
        "CS323 - Assignment 1\n",
        "Author: [Byran Peraza, Mohammad Ali Khan, Wen Fan ]\n",
        "Date:   [2/24/26]\n",
        "\n",
        "FSM-based lexer. Uses DFSMs for:\n",
        "  - Identifiers\n",
        "  - Integers\n",
        "  - Reals\n",
        "All other token types are handled ad-hoc.\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "#  Language definition for Rat26S\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "KEYWORDS = {\n",
        "    \"while\", \"if\", \"else\", \"endif\", \"return\",\n",
        "    \"get\", \"put\", \"integer\", \"real\", \"boolean\",\n",
        "    \"true\", \"false\", \"function\", \"for\", \"endwhile\"\n",
        "}\n",
        "\n",
        "OPERATORS = {\"==\", \"!=\", \"<=\", \">=\", \"<\", \">\", \"=\", \"+\", \"-\", \"*\", \"/\"}\n",
        "\n",
        "SEPARATORS = {\"(\", \")\", \"{\", \"}\", \"[\", \"]\", \",\", \";\", \":\"}\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "#  DFSM: Identifier\n",
        "#  RE: [a-zA-Z][a-zA-Z0-9_]*\n",
        "#\n",
        "#  States:\n",
        "#    0 = start\n",
        "#    1 = accepted (saw a letter)\n",
        "#    -1 = dead\n",
        "#\n",
        "#  Transitions:\n",
        "#    State 0 --letter--> 1\n",
        "#    State 1 --letter/digit/underscore--> 1\n",
        "#    anything else --> dead\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "def dfsm_identifier(s):\n",
        "    \"\"\"\n",
        "    Run the identifier DFSM on string s.\n",
        "    Returns the length of the longest valid identifier prefix, or 0.\n",
        "    \"\"\"\n",
        "    state = 0\n",
        "    i = 0\n",
        "    last_accept = 0  # no accept yet\n",
        "\n",
        "    while i < len(s):\n",
        "        ch = s[i]\n",
        "        if state == 0:\n",
        "            if ch.isalpha():\n",
        "                state = 1\n",
        "            else:\n",
        "                break  # dead\n",
        "        elif state == 1:\n",
        "            if ch.isalpha() or ch.isdigit() or ch == '_':\n",
        "                state = 1  # stay\n",
        "            else:\n",
        "                break  # done\n",
        "        i += 1\n",
        "        if state == 1:\n",
        "            last_accept = i\n",
        "\n",
        "    return last_accept\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "#  DFSM: Integer\n",
        "#  RE: [0-9]+\n",
        "#\n",
        "#  States:\n",
        "#    0 = start\n",
        "#    1 = accepted (saw a digit)\n",
        "#    -1 = dead\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "def dfsm_integer(s):\n",
        "    \"\"\"\n",
        "    Run the integer DFSM on string s.\n",
        "    Returns the length of the longest valid integer prefix, or 0.\n",
        "    \"\"\"\n",
        "    state = 0\n",
        "    i = 0\n",
        "    last_accept = 0\n",
        "\n",
        "    while i < len(s):\n",
        "        ch = s[i]\n",
        "        if state == 0:\n",
        "            if ch.isdigit():\n",
        "                state = 1\n",
        "            else:\n",
        "                break\n",
        "        elif state == 1:\n",
        "            if ch.isdigit():\n",
        "                state = 1\n",
        "            else:\n",
        "                break\n",
        "        i += 1\n",
        "        if state == 1:\n",
        "            last_accept = i\n",
        "\n",
        "    return last_accept\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "#  DFSM: Real\n",
        "#  RE: [0-9]+\\.[0-9]+\n",
        "#\n",
        "#  States:\n",
        "#    0 = start\n",
        "#    1 = digits before dot\n",
        "#    2 = saw dot  (not yet accepted)\n",
        "#    3 = digits after dot  (accepted)\n",
        "#    -1 = dead\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "def dfsm_real(s):\n",
        "    \"\"\"\n",
        "    Run the real DFSM on string s.\n",
        "    Returns the length of the longest valid real prefix, or 0.\n",
        "    \"\"\"\n",
        "    state = 0\n",
        "    i = 0\n",
        "    last_accept = 0\n",
        "\n",
        "    while i < len(s):\n",
        "        ch = s[i]\n",
        "        if state == 0:\n",
        "            if ch.isdigit():\n",
        "                state = 1\n",
        "            else:\n",
        "                break\n",
        "        elif state == 1:\n",
        "            if ch.isdigit():\n",
        "                state = 1\n",
        "            elif ch == '.':\n",
        "                state = 2\n",
        "            else:\n",
        "                break\n",
        "        elif state == 2:\n",
        "            if ch.isdigit():\n",
        "                state = 3\n",
        "            else:\n",
        "                break\n",
        "        elif state == 3:\n",
        "            if ch.isdigit():\n",
        "                state = 3\n",
        "            else:\n",
        "                break\n",
        "        i += 1\n",
        "        if state == 3:\n",
        "            last_accept = i\n",
        "\n",
        "    return last_accept\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "#  Token record\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "class Token:\n",
        "    def __init__(self, token_type, lexeme):\n",
        "        self.token_type = token_type\n",
        "        self.lexeme = lexeme\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Token({self.token_type!r}, {self.lexeme!r})\"\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "#  Lexer\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "class Lexer:\n",
        "    def __init__(self, source: str):\n",
        "        self.source = source\n",
        "        self.pos = 0\n",
        "\n",
        "    def _skip_whitespace_and_comments(self):\n",
        "        \"\"\"Skip spaces, tabs, newlines, and /* ... */ comments.\"\"\"\n",
        "        while self.pos < len(self.source):\n",
        "            # Whitespace\n",
        "            if self.source[self.pos].isspace():\n",
        "                self.pos += 1\n",
        "                continue\n",
        "\n",
        "            # Block comment  /* ... */\n",
        "            if self.source[self.pos:self.pos + 2] == \"/*\":\n",
        "                end = self.source.find(\"*/\", self.pos + 2)\n",
        "                if end == -1:\n",
        "                    raise SyntaxError(f\"Unterminated comment at position {self.pos}\")\n",
        "                self.pos = end + 2\n",
        "                continue\n",
        "\n",
        "            break  # nothing left to skip\n",
        "\n",
        "    def next_token(self):\n",
        "        \"\"\"Return the next Token, or None at end of input.\"\"\"\n",
        "        self._skip_whitespace_and_comments()\n",
        "\n",
        "        if self.pos >= len(self.source):\n",
        "            return None\n",
        "\n",
        "        remaining = self.source[self.pos:]\n",
        "\n",
        "        # ── Try Real DFSM first (must beat Integer since both start with digits) ──\n",
        "        real_len = dfsm_real(remaining)\n",
        "        if real_len > 0:\n",
        "            lexeme = remaining[:real_len]\n",
        "            self.pos += real_len\n",
        "            return Token(\"real\", lexeme)\n",
        "\n",
        "        # ── Try Integer DFSM ──\n",
        "        int_len = dfsm_integer(remaining)\n",
        "        if int_len > 0:\n",
        "            lexeme = remaining[:int_len]\n",
        "            self.pos += int_len\n",
        "            return Token(\"integer\", lexeme)\n",
        "\n",
        "        # ── Try Identifier / Keyword DFSM ──\n",
        "        id_len = dfsm_identifier(remaining)\n",
        "        if id_len > 0:\n",
        "            lexeme = remaining[:id_len]\n",
        "            self.pos += id_len\n",
        "            if lexeme in KEYWORDS:\n",
        "                return Token(\"keyword\", lexeme)\n",
        "            return Token(\"identifier\", lexeme)\n",
        "\n",
        "        # ── Ad-hoc: two-character operators ──\n",
        "        two = remaining[:2]\n",
        "        if two in OPERATORS:\n",
        "            self.pos += 2\n",
        "            return Token(\"operator\", two)\n",
        "\n",
        "        # ── Ad-hoc: one-character operators ──\n",
        "        one = remaining[0]\n",
        "        if one in OPERATORS:\n",
        "            self.pos += 1\n",
        "            return Token(\"operator\", one)\n",
        "\n",
        "        # ── Ad-hoc: separators ──\n",
        "        if one in SEPARATORS:\n",
        "            self.pos += 1\n",
        "            return Token(\"separator\", one)\n",
        "\n",
        "        # ── Unknown character ──\n",
        "        self.pos += 1\n",
        "        return Token(\"unknown\", one)\n",
        "\n",
        "    def tokenize(self):\n",
        "        \"\"\"Return a list of all tokens in the source.\"\"\"\n",
        "        tokens = []\n",
        "        while True:\n",
        "            tok = self.next_token()\n",
        "            if tok is None:\n",
        "                break\n",
        "            tokens.append(tok)\n",
        "        return tokens\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "#  Main program\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "def process_file(input_path: str, output_path: str):\n",
        "    with open(input_path, \"r\") as f:\n",
        "        source = f.read()\n",
        "\n",
        "    lexer = Lexer(source)\n",
        "\n",
        "    lines = []\n",
        "    header = f\"{'token':<15} {'lexeme'}\"\n",
        "    separator_line = \"-\" * 35\n",
        "    lines.append(header)\n",
        "    lines.append(separator_line)\n",
        "\n",
        "    while True:\n",
        "        tok = lexer.next_token()\n",
        "        if tok is None:\n",
        "            break\n",
        "        lines.append(f\"{tok.token_type:<15} {tok.lexeme}\")\n",
        "\n",
        "    output = \"\\n\".join(lines)\n",
        "    print(output)\n",
        "\n",
        "    with open(output_path, \"w\") as f:\n",
        "        f.write(output + \"\\n\")\n",
        "\n",
        "    print(f\"\\n[Output written to {output_path}]\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    if len(sys.argv) < 2 or sys.argv[1].startswith('-'):\n",
        "        print(\"Running built-in test cases...\")\n",
        "        run_tests()\n",
        "    elif sys.argv[1] == \"--test\":\n",
        "        run_tests()\n",
        "    else:\n",
        "        input_path = sys.argv[1]\n",
        "        output_path = sys.argv[2] if len(sys.argv) > 2 else \"output.txt\"\n",
        "        process_file(input_path, output_path)\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "#  Built-in test cases\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "TEST_CASES = {\n",
        "   \"test1.rat\": \"\"\"\n",
        "/* Compute the average of two real numbers */\n",
        "function average(a, b) {\n",
        "    real result;\n",
        "    result = (a + b) / 2.00;\n",
        "    return result;\n",
        "}\n",
        "\"\"\" ,\n",
        "\"test2.rat\": \"\"\"\n",
        "/* Count down from n to zero */\n",
        "integer n;\n",
        "n = 10;\n",
        "while (n > 0) {\n",
        "    put(n);\n",
        "    n = n - 1;\n",
        "}\n",
        "endwhile\n",
        "\"\"\",\n",
        "\"test3.rat\": \"\"\"\n",
        "/* Grade checker: checks score and prints pass or fail */\n",
        "integer score;\n",
        "boolean passed;\n",
        "get(score);\n",
        "if (score >= 60) {\n",
        "    passed = true;\n",
        "} else {\n",
        "    passed = false;\n",
        "}\n",
        "endif\n",
        "put(passed);\n",
        "\"\"\",\n",
        "}\n",
        "\n",
        "\n",
        "def run_tests():\n",
        "    os.makedirs(\"test_outputs\", exist_ok=True)\n",
        "\n",
        "    for filename, source in TEST_CASES.items():\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"TEST CASE: {filename}\")\n",
        "        print(f\"Source:\\n{source.strip()}\\n\")\n",
        "\n",
        "        lexer = Lexer(source)\n",
        "        tokens = lexer.tokenize()\n",
        "\n",
        "        print(f\"{'token':<15} {'lexeme'}\")\n",
        "        print(\"-\" * 35)\n",
        "        for tok in tokens:\n",
        "            print(f\"{tok.token_type:<15} {tok.lexeme}\")\n",
        "\n",
        "        # Write to output file\n",
        "        out_path = os.path.join(\"test_outputs\", filename.replace(\".rat\", \"_output.txt\"))\n",
        "        with open(out_path, \"w\") as f:\n",
        "            f.write(f\"Source: {filename}\\n\")\n",
        "            f.write(f\"{'token':<15} {'lexeme'}\\n\")\n",
        "            f.write(\"-\" * 35 + \"\\n\")\n",
        "            for tok in tokens:\n",
        "                f.write(f\"{tok.token_type:<15} {tok.lexeme}\\n\")\n",
        "\n",
        "        print(f\"\\n[Written to {out_path}]\")\n",
        "        print()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}